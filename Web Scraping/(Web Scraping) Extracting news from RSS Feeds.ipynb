{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting news from publicly-available RSS Feeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the extraction of news from publicly-available RSS Feeds. **For showcasing purposes, the providers and URLs are removed.** The objective of this function is to extract news from such feeds for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re \n",
    "import html\n",
    "import os \n",
    "import datetime\n",
    "\n",
    "urls = {'XXX' : \"XXX\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RSS feeds are based on XML. Using BeautifulSoup's in-built XML parser, we will be able to find the information we want based on the tags. If any of the tags are not available (eg. description), we will discard that article, adding to *skip_count*, which occurs occasionally. \n",
    "\n",
    "For each different source, we will pre-process the incoming articles with different regular expressions (regex). This is because each source formats their RSS feed differently, thus the need to pre-process them individually. \n",
    "\n",
    "Once pre-processed, compile all article into a CSV file:\n",
    "1. Append if news.csv exist : slice df based on existence of link in csv file into 'update'\n",
    "2. Create a new news.csv file if it does not exist. \n",
    "\n",
    "First, we attempt to open the 'news.csv' file. If the file does not exist, we will create the file when we export our first set of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading of CSV file successful.\n"
     ]
    }
   ],
   "source": [
    "new_count, skip_count, create_file = 0, 0, 0\n",
    "try:\n",
    "    read = pd.read_csv('news.csv', sep = ',')\n",
    "    print(\"Reading of CSV file successful.\")\n",
    "    link = read['link']\n",
    "except:\n",
    "    create_file = 1 # Create a new news.csv file if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(urls)):\n",
    "    source = list(urls.keys())[n] \n",
    "    url = list(urls.values())[n] \n",
    "        \n",
    "    try:\n",
    "        resp = requests.get(url) # To handle any invalid URLs that cannot be requested \n",
    "    except:\n",
    "        print('Invalid URL: {}'.format(url))\n",
    "        continue\n",
    "        \n",
    "    soup = BeautifulSoup(resp.content, features= 'xml')\n",
    "    items = soup.find_all('item')\n",
    "    news = []\n",
    "    for item in items:\n",
    "        try:\n",
    "            i = {}\n",
    "            i['title'] = html.unescape(item.title.text)\n",
    "            i['description'] = html.unescape(item.description.text)\n",
    "            i['link'] = item.link.text\n",
    "            i['pubDate'] = pd.to_datetime(item.pubDate.text).tz_convert(None).normalize()\n",
    "            i['source'] = source\n",
    "            news.append(i)\n",
    "        except:\n",
    "            skip_count += 1\n",
    "            continue\n",
    "        \n",
    "    df = pd.DataFrame(news, columns = ['title', 'description', 'link', 'pubDate', 'source'])\n",
    "        \n",
    "    \"\"\"\n",
    "    For each different source, pre-process the incoming news with individual Regex.\n",
    "    This is because each source formats their RSS feed differently. \n",
    "    \"\"\"\n",
    "    if source in ['XXX', 'XXA', 'XXB', 'XXC']:\n",
    "        regex = '[ ]?\\\\.\\\\.\\\\.'\n",
    "        for i in range(len(df)):\n",
    "            df.loc[i, 'description'] = re.sub(regex, '', df.loc[i, 'description'])\n",
    "                \n",
    "    if create_file != 1:\n",
    "        update = df.loc[df['link'].isin(link.values) == False]\n",
    "        if len(update) != 0:\n",
    "            new_count += len(update)\n",
    "            update.to_csv('news.csv', mode = 'a', index = False, header = False)\n",
    "            print('Updated on {} for {} with {} new article(s).'.format(datetime.datetime.now(), source, len(update)))\n",
    "    else:\n",
    "        df.to_csv('news.csv', mode = 'w', index = False, sep = ',')\n",
    "        create_file = 0\n",
    "        print('File created: {}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of items skipped: 0 \n",
      "New article(s): 0 \n",
      "Total articles: 100\n"
     ]
    }
   ],
   "source": [
    "print('No. of items skipped: {}'.format(skip_count),'\\nNew article(s): {}'.format(new_count), '\\nTotal articles: {}'.format(len(read)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

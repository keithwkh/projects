{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting market movement on D+1 with current day news\n",
    "\n",
    "In this project, we are going to explore the possibility of predicting a market's movement using current-day news articles by using Natural Language Processing and Machine Learning techniques.\n",
    "\n",
    "##### This project was last updated on 16 Aug 2020. The author concluded that the model is not able to predict the market's movement on D+1 based on news released from a single source on D. For more details, please see the Conclusion section. This project will not be updated anymore. Note that neither the author (Keith Wee) nor the results of this project represents any political or religious view or opinion. \n",
    "\n",
    "### Dataset information \n",
    "Using RSS feeds that are publicly available, we regularly run our web scraping script (available on our GitHub repository) to extract new articles that are periodically updated by the provider. We decided to run the script every morning 8AM, afternoon 3PM and night 9PM. Over the course of one month, we collected 692 articles from a single RSS feed that focuses on Singapore's news. *(16 Aug 2020) In total, we collected 1115 articles for use in this project.*\n",
    "\n",
    "**All articles used in this project belongs to its copyright owners. My use of these resources constitutes personal and non-commercial use.**\n",
    "\n",
    "### Objective\n",
    "To predict the impact of current day's news articles on the stock market (upward or downward movement) tomorrow in Singapore.\n",
    "\n",
    "### Approach\n",
    "We will be using the **TfidfVectorizer** module by Scikit-Learn and the **Bag-of-words** approach. TfidfVectorizer is a form of Feature Extraction whereby it transforms text data into features vector that can be used as input for estimators. The value of a word is proportionate to the count in the corpus (a collection of texts), adjusted inversely to the number of documents it appears in the corpus, also known as Inverse Document Frequency (IDF). IDF adjusts words that frequently appears across multiple documents / texts (e.g. we, the, I), which will not be useful in drawing differences between documents.\n",
    "\n",
    "To replicate real-life situation as much as possible, we decided to fit the vectorizer using a time-series forecasting approach. This means that we will fit the vectorizer with data prior to a cut-off date, instead of using train_test_split (which is random and may include \"future\" data). Essentially, we assume the appearance of a particular word each day will affect the market's movement the next day.\n",
    "\n",
    "### Assumptions\n",
    "We make the following assumptions in this project:\n",
    "1. The market is NOT semi-strong form efficient, which in turn means it is not strong-form efficient.\n",
    "2. The appearance of a single word in an article has, to a certain extent, an impact (positive or negative) to the market.\n",
    "3. The stock market movement is purely functional on news articles from a single source.\n",
    "4. All market players act the same way when consuming these information. \n",
    "\n",
    "The first assumption is necessary as if the market is semi-strong form efficient, the moment the article is released, the impact will be reflected immediately in the market the day the article is released (unless it is after market closure). Assumptions 3 and 4 are not realistic but are essential for this project.\n",
    "\n",
    "*Updated on 16 August 2020 to amend an error in assumptions made. However, there are no impact to the conclusions drawn previously.*\n",
    "\n",
    "### Model\n",
    "In this project, we decided to use the Logistic Regression model as we are looking at a binary scenario. In the future, we will be further exploring the use of other models (e.g. MultinomialNB, Regression models) for multi-class classification or even regression analysis. \n",
    "\n",
    "In the attempt to achieve the highest accuracy and AUROC on the testing data, we modified the C regularisation parameter. For more information on regularisation in machine learning, see here: https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a . \n",
    "\n",
    "#### Measures\n",
    "The classifier will be evaluated with the following metrics: \n",
    "\n",
    " - Precision score\n",
    " - Recall score\n",
    " - AUROC\n",
    " - Accuracy score for the testing set and training set\n",
    " \n",
    "### Conclusion\n",
    "\n",
    "#### 19th July 2020\n",
    "We managed to achieve an accuracy score of 0.7272, which is the highest we have seen well attempting different methods offline. It has a relatively high recall of 0.75 and AUROC of 0.73214. However, we see a low precision score of 0.6000. This Type I error may be costly to investors if they depend on this model to guide their investment decision. Such decisions may include investing when given a positive signal (1) and the market moves down tomorrow (a false positive). \n",
    "\n",
    "It is also worth a note that the perfect training accuracy is due to the model learning the vocabulary and coefficients from the training set itself. However, this may be signs of overfitting on the training data. In the future, with more data, we hope to do a Train-Evaluate-Test split on our dataset, which will give us better insight to the model.\n",
    "\n",
    "As mentioned in our approach, we assume that the appearance of a single word today influences the market as a whole the next day to a certain extent. This brings us to our first limitation of the model: **it does not take into context of how the word is used**. For example, compare the sentences *\"Bars in Singapore are heavily impacted due to COVID-19\"* and *\"Bars are allowed to resume operations next week.\"* Ignoring all other terms, the classifier will predict a downward market movement tomorrow simply due to the word \"bar\" *(see Negative Determinants)*, despite that the contexts are largely different. We can improve this model using n-grams of a word (e.g bigrams or trigrams) in its original sentence, which will provide a better context. The downside of doing so is that it exponentially increases the number of features extracted by the Vectorizer, and it has to be done at a document-level (e.g generate n-grams tokens for each article and consolidate them together at the end for each date). Thus, we suggest the use of *min_df* parameter in the Vectorizer when fitting it with n-grams features. This parameter suggests that tokens that do not appear in more than, or equal to, $X$ documents shall be ignored (e.g if min_df = 2, a token/word has to appear in at least 2 documents for it to be a feature). This will significantly reduce the amount of features passed into the classifier. This can also potentially cut out misspelled words or words that are unique to a certain article. \n",
    "\n",
    "Another limitation is that the classifier is only trained on words seen by the Vectorizer. A simple google search tells us that the Oxford dictionary contains 171,476 words, and this does not include *slangs* (short languages) and acronyms, among others. In our Vectorizer, the vocabulary only contains 2104 words. This means that when faced with new words, the Vectorizer simply ignores them and fit the words that it have seen. With our limited dataset, it was surprising to see such high accuracy and AUROC. During our drafting, we adjusted the amount of training data (aka fitting data) that is seen by the Vectorizer and Classifier. We noticed that as the amount of training data increases (i.e more vocabulary), the accuracy of the classifier drops (to approximate 0.6) on the testing set. We will continue to work on getting more data (both training and testing) and continue to see if predicting stock market movement purely based on news article is sufficient.\n",
    "\n",
    "As we continue to work on this project, we will occasionally update the conclusion we have arrived at.\n",
    "\n",
    "#### 16 August 2020\n",
    "Since 19 July 2020, we continued to harvest our sources at regular interval for the purpose of this project. In total, we scraped 1116 articles from a single source, where only 748 are used in this project (eg. only Sunday's articles are used to predict Monday's market movements, ignoring all articles from Friday and Saturday). \n",
    "\n",
    "As we continued to gather more data over the past 2 months for the project, we also continued to train our model. Using the same model (eg. without improving or implementing the considerations mentioned on 19 July 2020), we concluded that using the bag-of-words approach on articles from a single source on D to predict market's movement on D+1 is **ineffective**. With more data collected, we trained the model with a 75/25 train-test split using time-series forecasting approach.\n",
    "\n",
    "The results were expected by us since the beginning. The Area Under the Receiver Operating Characteristic Curve (AUROC) drops to 0.5, signalling that the model is **incapable** of distinguishing the differences between the binary classes. \n",
    "\n",
    "In addition to the considerations mentioned in our conclusion on 19 July, we also studied the way we collected our data. First, most RSS feeds provide a timezone-aware timestamp for the datetime the news is released. However in our process of web-scraping, we removed such essential information and normalised the timestamp to midnight of the day the article is released. This gave rise to our assumption of the lack of semi-strong form efficiency in the market. However, with the recent advance in technology, algorithmic trading based on real-time structured and unstructured data has become very common. Such technology analyses, prices, and takes advantage of any price discrepancies almost instantly. As such, we should not assume that the market is semi-strong form inefficient. Therefore, we should **only consider articles released after market closure** as market players would not be able to price these articles until the next market-open day. Thus, a timezone-aware datetimestamp will be needed. Next, we should also consider combining articles from multiple sources that are relevant to a target market. In this project, only articles from a single source are used. This is yet another unrealistic assumption that all market players consume only information from a single source.\n",
    "\n",
    "All in all, the project demonstrated the use of Natural Language Processing and Supervised Machine Learning techniques using Python. The results we arrived at were expected due to the unrealistic assumptions. However, this gave us the opportunity to reflect and think about how to improve the way we collect, clean and manage our data. This project also allowed us to think about how such techniques can be used in the areas of finance. With this, this project shall be deemed concluded and will not be further worked on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = pd.read_csv('C:/XXX/news.csv', sep = ',') #masked for privacy\n",
    "read.sort_values('pubDate', ascending = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing\n",
    "In this section, we will process the scrapped articles by title, description, and publication date. First, we call the generate_dataFrame() function. This function consolidates a strings of tokens for each day into a Pandas Dataframe by calling the other functions: generate_tokens() and process_text(). \n",
    "\n",
    "To start off, the process_text() function takes in a series where each item in the series is tokenized with punctuations removed. If a token consists of just punctuation(s), it will be removed and discarded. Following that, we attempted to lemmatize each token to achieve standardisation. We first do Part-of-Speech (POS) tagging on the generated tokens. POS tagging is essential in the lemmatization process and it is used as a required argument in the Lemmatizer. Finally, we proceed to lemmatize these tokens in the list, which will be passed to the generate_tokens() function. \n",
    "\n",
    "In generate_tokens(), we filter out stopwords which is not meaninful as it appears in almost all documents. Then, we use only alpha tokens as numeric tokens does not provide enough context (e.g. 50 apples in a basket, 50 injuries in a car accident) by itself. These tokens are then consolidated by date, and passed back to the dataFrame, together with its date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(series):\n",
    "    lst = []\n",
    "    for item in series:\n",
    "        tokens = nltk.word_tokenize(item)\n",
    "        tokens = [token.translate(str.maketrans('','', string.punctuation)).lower() for token in tokens if token.translate(str.maketrans('','', string.punctuation)) != '']\n",
    "        lst = lst + tokens\n",
    "                                \n",
    "    pos_tag = dict(nltk.pos_tag(lst))\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    def wn_pos_type(token):\n",
    "        if token in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "            return nltk.corpus.wordnet.NOUN\n",
    "        elif token in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "            return nltk.corpus.wordnet.VERB\n",
    "        elif token in ['RB', 'RBR', 'RBS']:\n",
    "            return nltk.corpus.wordnet.ADV\n",
    "        elif token in ['JJ', 'JJR', 'JJS']:\n",
    "            return nltk.corpus.wordnet.ADJ\n",
    "        else:\n",
    "            return 'n' # Default parameter by WordNetLemmatizer. Returns the word itself.\n",
    "    lst = [lemmatizer.lemmatize(tok, pos = wn_pos_type(pos_tag[tok])) for tok in lst]\n",
    "    return lst\n",
    "\n",
    "def generate_tokens(df):\n",
    "    title_series = df['title']\n",
    "    desc_series = df['description']\n",
    "    tokens = process_text(title_series) + process_text(desc_series)\n",
    "    tokens = [tok for tok in tokens if tok not in stopwords.words('english') and tok.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "def generate_dataFrame():\n",
    "    tokens_list = []\n",
    "    for date in list(read['pubDate'].unique()):\n",
    "        dataFrame = read[(read['pubDate'] == date) & (read['source'] == 'XXX_XXX')]\n",
    "        tokens = generate_tokens(dataFrame)\n",
    "        tokens_list.append([date, \" \".join(tokens)])\n",
    "            \n",
    "    df = pd.DataFrame(tokens_list, columns = ['date', 'tokens'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-06-17</th>\n",
       "      <td>nursing home resident allow visitor day jun tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-18</th>\n",
       "      <td>paul tambyah first singaporean head internatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-19</th>\n",
       "      <td>playgrounds beach reopen start phase patient a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       tokens\n",
       "date                                                         \n",
       "2020-06-17  nursing home resident allow visitor day jun tw...\n",
       "2020-06-18  paul tambyah first singaporean head internatio...\n",
       "2020-06-19  playgrounds beach reopen start phase patient a..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = generate_dataFrame()\n",
    "df.set_index('date', inplace = True)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market Movement\n",
    "The market index we chose in this project is the FTSE Straits Times Index (STI). Historical data is extracted from Yahoo Finance. Using in-built functions, we calculated the percentage change of the closing point and offset the date by -1. This is because of our objective to find out how news today affects the markets tomorrow (e.g. how will news on 3rd July 2020 affects the market movement on 4th July 2020). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sti = pd.read_csv('C:/XXX/STI.csv').ffill() #masked for privacy\n",
    "sti['Date'] = pd.to_datetime(sti['Date'])\n",
    "sti.set_index('Date', inplace = True)\n",
    "sti['Chng'] = sti.pct_change(periods = 1)['Close']\n",
    "sti.index = sti.index - pd.DateOffset(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, we would like to explain how, and why, we merged the Dataframes in the next block. We want to raise a note that we are essentially predicting Monday's market movement using Sunday's articles only, completely ignoring Friday's and Saturday's news. Realistically, this is not the case as investors consumes news over the weekend and price it when market opens on Monday. Having said that, since we offset the dates by -1 (the percentage change on 13th Jul becomes the target for 12th Jul), we merged the Dataframes together to produce a complete Dataframe with input (the tokenized articles) and output (the pct_change). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results from 19th July 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Positive Determinants in July \n",
      "      word      coef\n",
      "0  monday  0.698438\n",
      "1   party  0.641720\n",
      "2     pap  0.555946\n",
      "3     grc  0.436361\n",
      "4     jul  0.436258\n",
      "\n",
      "Top 5 Neagtive Determinants in July \n",
      "         word      coef\n",
      "2099  mosque -0.272058\n",
      "2100  friday -0.280093\n",
      "2101     jun -0.346791\n",
      "2102     bar -0.347927\n",
      "2103  sunday -0.377431\n",
      "\n",
      "July Precision: 0.6\n",
      "July Recall: 0.75\n",
      "July AUROC: 0.7321428571428571\n",
      "July Accuracy (Test): 0.7272727272727273\n",
      "July Accuracy (Train): 1.0\n"
     ]
    }
   ],
   "source": [
    "merged_df = df.merge(sti, how = 'inner', left_index = True, right_index = True).sort_index(ascending = True)\n",
    "\"\"\"On 19th Jul, we used approximately half of our dataset to train and half to test. This is because we carry out live daily scrapping of news from various sources and do not have ready access to historical news sources.\"\"\"\n",
    "training_jul = merged_df[merged_df.index <= pd.to_datetime('1 July 2020')]\n",
    "testing_jul = merged_df[(merged_df.index > pd.to_datetime('1 July 2020')) & (merged_df.index <= pd.to_datetime('19 July 2020'))].dropna(how = 'all')\n",
    "\n",
    "\"\"\"We conduct a binary classification using existing data, where 1 represents a positive change and 0 represents a negative change\"\"\"\n",
    "X_train_jul = training_jul['tokens']\n",
    "y_train_jul = np.where(training_jul['Chng'] >= 0, 1, 0)\n",
    "\n",
    "X_test_jul = testing_jul['tokens']\n",
    "y_test_jul = np.where(testing_jul['Chng'] >= 0, 1, 0)\n",
    "\n",
    "vect_jul = TfidfVectorizer().fit(X_train_jul)\n",
    "X_train_vectorized_jul = vect_jul.transform(X_train_jul)\n",
    "clf_jul = LogisticRegression(C = 5).fit(X_train_vectorized_jul, y_train_jul) # C = 5 generates the best accuracy and AUROC for the test set after several experiments\n",
    "\n",
    "coeff_jul = clf_jul.coef_\n",
    "idx_jul = np.argsort(-coeff_jul, ).reshape(-1, 1)\n",
    "vocab_jul = vect_jul.vocabulary_\n",
    "\n",
    "word_list_jul = []\n",
    "for i in idx_jul:\n",
    "    for word, n in vocab_jul.items():\n",
    "        if n == i:\n",
    "            word_list_jul.append([word, coeff_jul.reshape(-1, 1)[i][0, 0]])\n",
    "word_list_jul = pd.DataFrame(word_list_jul, columns = [\"word\", \"coef\"])\n",
    "\n",
    "print(\"Top 5 Positive Determinants in July \\n\", word_list_jul.head(5))\n",
    "print(\"\\nTop 5 Neagtive Determinants in July \\n\", word_list_jul.tail(5))\n",
    "\n",
    "predictions_jul = clf_jul.predict(vect_jul.transform(X_test_jul))\n",
    "\n",
    "y_test_jul= y_test_jul.reshape(-1, 1)\n",
    "precision_jul = precision_score(y_test_jul, predictions_jul)\n",
    "recall_jul = recall_score(y_test_jul, predictions_jul)\n",
    "accuracy_jul = clf_jul.score(vect_jul.transform(X_test_jul), y_test_jul)\n",
    "roc_jul = roc_auc_score(y_test_jul, predictions_jul)\n",
    "\n",
    "print(\"\\nJuly Precision: {}\\nJuly Recall: {}\\nJuly AUROC: {}\\nJuly Accuracy (Test): {}\\nJuly Accuracy (Train): {}\".format(precision_jul, recall_jul, roc_jul, accuracy_jul, clf_jul.score(X_train_vectorized_jul, y_train_jul)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results from 16th August 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Positive Determinants in August: \n",
      "         word      coef\n",
      "0  candidate  0.824228\n",
      "1      party  0.778829\n",
      "2        pap  0.619303\n",
      "3         ng  0.544373\n",
      "4     return  0.540972\n",
      "\n",
      "Top 5 Negative Determinants in August: \n",
      "            word      coef\n",
      "3711  transport -0.320686\n",
      "3712     sunday -0.361915\n",
      "3713     arrest -0.400221\n",
      "3714        bar -0.400547\n",
      "3715  wednesday -0.498875\n",
      "\n",
      "August Precision: 0.0\n",
      "August Recall: 0.0\n",
      "August AUROC: 0.5\n",
      "August Accuracy (Test): 0.4444444444444444\n",
      "August Accuracy (Train): 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"On 16 Aug, we used the 75/25 (based on number of days) train-test split through the time-series forecasting approach\"\"\"\n",
    "training_aug = merged_df[merged_df.index <= pd.to_datetime('31 July 2020')]\n",
    "testing_aug = merged_df[~merged_df.isin(training_aug)].dropna(how = 'all')\n",
    "\n",
    "X_train_aug = training_aug['tokens']\n",
    "y_train_aug = np.where(training_aug['Chng'] >= 0, 1, 0)\n",
    "\n",
    "X_test_aug = testing_aug['tokens']\n",
    "y_test_aug = np.where(testing_aug['Chng'] >= 0, 1, 0)\n",
    "\n",
    "vect_aug = TfidfVectorizer().fit(X_train_aug)\n",
    "X_train_vectorized_aug = vect_aug.transform(X_train_aug)\n",
    "clf_aug = LogisticRegression(C = 5).fit(X_train_vectorized_aug, y_train_aug) # C = 5 to remain consistent with the model trained in July 2020\n",
    "\n",
    "coeff_aug = clf_aug.coef_\n",
    "idx_aug = np.argsort(-coeff_aug, ).reshape(-1, 1)\n",
    "vocab_aug = vect_aug.vocabulary_\n",
    "\n",
    "word_list_aug = []\n",
    "for i in idx_aug:\n",
    "    for word, n in vocab_aug.items():\n",
    "        if n == i:\n",
    "            word_list_aug.append([word, coeff_aug.reshape(-1, 1)[i][0, 0]])\n",
    "word_list_aug = pd.DataFrame(word_list_aug, columns = [\"word\", \"coef\"])\n",
    "\n",
    "print(\"Top 5 Positive Determinants in August: \\n\", word_list_aug.head(5))\n",
    "print(\"\\nTop 5 Negative Determinants in August: \\n\", word_list_aug.tail(5))\n",
    "\n",
    "predictions_aug = clf_aug.predict(vect_aug.transform(X_test_aug))\n",
    "\n",
    "y_test_aug = y_test_aug.reshape(-1, 1)\n",
    "precision_aug = precision_score(y_test_aug, predictions_aug)\n",
    "recall_aug = recall_score(y_test_aug, predictions_aug)\n",
    "accuracy_aug = clf_aug.score(vect_aug.transform(X_test_aug), y_test_aug)\n",
    "roc_aug = roc_auc_score(y_test_aug, predictions_aug)\n",
    "\n",
    "print(\"\\nAugust Precision: {}\\nAugust Recall: {}\\nAugust AUROC: {}\\nAugust Accuracy (Test): {}\\nAugust Accuracy (Train): {}\".format(precision_aug, recall_aug, roc_aug, accuracy_aug, clf_aug.score(X_train_vectorized_aug, y_train_aug)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error message \"UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\" was generated as the predictions in August returned a single class (eg. all predictions were 0). This is because the classifier cannot determine the differences between the binary classes (as seen with an AUROC of 0.5) therefore the it predicts 0 for all cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A final note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One disadvantage of using the bag-of-words approach is that the trained model can simply be **manipulated** to generate a positive or negative response by simply parsing in the top determinants (shown below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [1]. An example of a manipulated positive response\n",
      "Prediction: [0]. An example of a manipulated negative response\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction: {}. An example of a manipulated positive response\".format(clf_jul.predict(vect_jul.transform([\"monday party pap jul grc\"]))))\n",
    "print(\"Prediction: {}. An example of a manipulated negative response\".format(clf_jul.predict(vect_jul.transform([\"mosque friday jun bar sunday\"]))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
